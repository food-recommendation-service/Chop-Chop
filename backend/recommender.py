import requests
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import re
import time
import os
from dotenv import load_dotenv

load_dotenv()

# ==================================================================================
# [1] API í‚¤ ë° ëª¨ë¸ ì„¤ì •
# ==================================================================================
GOOGLE_API_KEY = "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s"
GEMINI_API_KEY = "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s"

genai.configure(api_key=GEMINI_API_KEY)
llm_model = genai.GenerativeModel('models/gemini-2.0-flash')

print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
embed_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")

# ==================================================================================
# [2] í•µì‹¬ ë¶„ì„ ë° ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ (ì›ë³¸ ë¡œì§ ìœ ì§€)
# ==================================================================================

def haversine_distance(lat1, lon1, lat2, lon2):
    """ë‘ ì§€ì ì˜ ìœ„ë„, ê²½ë„ë¥¼ ë°›ì•„ ê±°ë¦¬ë¥¼ km ë‹¨ìœ„ë¡œ ê³„ì‚°"""
    R = 6371
    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)
    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)
    dlon, dlat = lon2_rad - lon1_rad, lat2_rad - lat1_rad
    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

def calculate_recency_score(reviews):
    """[í™œì„±ë„ ì ìˆ˜] ë¦¬ë·° 5ê°œ ì´ìƒ ì‹œ ë§Œì """
    if not reviews: return 0.0
    return min(len(reviews) / 5.0, 1.0) 

def calculate_popularity_score(count):
    """[ì¸ê¸°ë„ ì ìˆ˜] ë¦¬ë·° ê°œìˆ˜ë¥¼ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜"""
    if not count: return 0.0
    return min(np.log10(count + 1) / 4.0, 1.0)

def hard_filter_by_similarity(place_docs, user_query, threshold=0.3):
    """
    [í•˜ë“œ í•„í„°ë§] ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ ë‚®ì€ ì‹ë‹¹ ì¦‰ì‹œ ì œê±°
    'ë‘ë°”ì´ì«€ë“ì¿ í‚¤' ì…ë ¥ ì‹œ ìœ ì‚¬ë„ 0.3 ë¯¸ë§Œì¸ ì‹ë‹¹ì€ ê°€ì°¨ì—†ì´ íƒˆë½ì‹œí‚µë‹ˆë‹¤.
    """
    if not place_docs: return []
    doc_texts = [p['text'] for p in place_docs]
    embeddings = embed_model.encode(doc_texts)
    query_embedding = embed_model.encode([user_query])
    sim_scores = cosine_similarity(query_embedding, embeddings)[0]
    
    passed_docs = []
    for i, score in enumerate(sim_scores):
        if score >= threshold:
            place_docs[i]['sim_score'] = score
            passed_docs.append(place_docs[i])
    
    print(f"âœ‚ï¸ í•˜ë“œ í•„í„°ë§: {len(place_docs)}ê°œ ì¤‘ {len(passed_docs)}ê°œ ìƒì¡´ (ê¸°ì¤€: {threshold})")
    return passed_docs

def get_naver_style_features(place_name, reviews):
    """[LLM ë¶„ì„] ë¦¬ë·°ì—ì„œ ë¶„ìœ„ê¸°, ë™í–‰, ëª©ì  ì¶”ì¶œ"""
    if not reviews: return {}
    combined_review = " ".join([r.get('text', {}).get('text', '') for r in reviews[:5]])
    prompt = f"""
    ë‹¹ì‹ ì€ ë§›ì§‘ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì‹ë‹¹ì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì—¬ ì •ë³´ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
    ì‹ë‹¹ëª…: {place_name}
    ë¦¬ë·°ë°ì´í„°: {combined_review[:800]}
    ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”: {{"atmosphere": "...", "companion": "...", "purpose": "...", "keywords": [...]}}
    """
    try:
        response = llm_model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except Exception: return {}

# ==================================================================================
# [3] ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (Pagination ì ìš©í•˜ì—¬ 200ê°œ í™•ë³´)
# ==================================================================================

def get_bulk_places(search_query, center_lat, center_lng, radius_km, target_count=200):
    """[ëŒ€ëŸ‰ ìˆ˜ì§‘] ìµœëŒ€ 10í˜ì´ì§€(200ê°œ)ê¹Œì§€ ë°˜ë³µ í˜¸ì¶œ"""
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.rating,places.userRatingCount,places.reviews,places.location,places.formattedAddress,nextPageToken'
    }
    all_places_dict = {}
    next_token = None
    
    print(f"ğŸ•µï¸ '{search_query}' ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)...")
    
    for page in range(10):
        payload = {
            "textQuery": search_query,
            "locationBias": {"circle": {"center": {"latitude": center_lat, "longitude": center_lng}, "radius": radius_km * 1000}},
            "languageCode": "ko", "maxResultCount": 20, "pageToken": next_token
        }
        try:
            response = requests.post(url, json=payload, headers=headers)
            data = response.json()
            places = data.get('places', [])
            
            added = 0
            for p in places:
                pid = p.get('id')
                if pid and pid not in all_places_dict:
                    all_places_dict[pid] = p
                    added += 1
            print(f"  ğŸ“„ {page+1}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... (+{added}ê°œ)")
            
            next_token = data.get('nextPageToken')
            if not next_token or len(all_places_dict) >= target_count: break
            time.sleep(1.0) # êµ¬ê¸€ API ë”œë ˆì´ ì¤€ìˆ˜
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break
            
    return list(all_places_dict.values())

# ==================================================================================
# [4] ë©”ì¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸
# ==================================================================================

def search_and_analyze(categories, user_detail, lat, lng, radius_km):
    category_str = " ".join(categories)
    search_query = f"{category_str} {user_detail}".strip()
    if not search_query: search_query = "ë§›ì§‘"

    # 1. 200ê°œ ëŒ€ëŸ‰ ìˆ˜ì§‘
    places = get_bulk_places(search_query, lat, lng, radius_km, target_count=200)
    if not places: return {"result": "âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "stores": []}

    # 2. ê±°ë¦¬ í•„í„°ë§ ë° ì „ì²˜ë¦¬
    filtered_places = []
    for p in places:
        loc = p.get('location', {})
        dist = haversine_distance(lat, lng, loc.get('latitude', 0), loc.get('longitude', 0))
        if dist <= radius_km:
            reviews = p.get('reviews', [])
            review_text = " ".join([r.get('text', {}).get('text', '') for r in reviews])
            filtered_places.append({
                "name": p.get('displayName', {}).get('text', 'ì´ë¦„ì—†ìŒ'),
                "rating": p.get('rating', 0),
                "count": p.get('userRatingCount', 0),
                "reviews": reviews,
                "text": review_text,
                "lat": loc.get('latitude'), "lng": loc.get('longitude'), "address": p.get('formattedAddress', '')
            })
    
    scanned_count = len(filtered_places)
    
    # 3. í•˜ë“œ í•„í„°ë§ (ì˜ë¯¸ì  ìœ ì‚¬ë„ 0.3 ê¸°ì¤€)
    valid_docs = [p for p in filtered_places if p['text'].strip()]
    candidates = hard_filter_by_similarity(valid_docs, search_query, threshold=0.3)
    analyzed_count = len(candidates)

    if not candidates: return {"result": "âš ï¸ ì¶©ë¶„íˆ ê´€ë ¨ ìˆëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.", "stores": []}

    # 4. ì¢…í•© ìŠ¤ì½”ì–´ë§ (ìœ ì‚¬ë„ 30, í‰ì  35, ë¦¬ë·°ìˆ˜ 25, ìµœì‹ ì„± 10)
    for p in candidates:
        p['total_score'] = (p['sim_score'] * 0.30) + (p['rating']/5 * 0.35) + (calculate_popularity_score(p['count']) * 0.25) + (calculate_recency_score(p['reviews']) * 0.10)
        p['match_rate'] = int(p['total_score'] * 100)

    # 5. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± (ìƒìœ„ 3ê°œ ë¶„ì„)
    top_3 = sorted(candidates, key=lambda x: x['total_score'], reverse=True)[:3]
    result_report = f"\n{'='*65}\nğŸ† '{search_query}' AI ì¶”ì²œ ë¦¬í¬íŠ¸ (ë¶„ì„ ëŒ€ìƒ: {len(candidates)}ê°œ)\n{'='*65}\n"
    stores_data = []
    
    for rank, p in enumerate(top_3, 1):
        features = get_naver_style_features(p['name'], p['reviews'])
        kws = ", ".join(features.get('keywords', [])) if features.get('keywords') else "ë¶„ì„ì¤‘..."
        
        result_report += f"ğŸ… {rank}ìœ„: {p['name']} (ë§¤ì¹­ {p['match_rate']}%)\n"
        result_report += f"   â­ï¸ í‰ì : {p['rating']}ì  | ë¦¬ë·° {p['count']}ê°œ\n"
        result_report += f"   ğŸ  ë¶„ìœ„ê¸°: {features.get('atmosphere', '-')} | ğŸ‘¥ ì¶”ì²œ: {features.get('companion', '-')}\n"
        result_report += f"   ğŸ¯ ëª©  ì : {features.get('purpose', '-')} | ğŸ”‘ í‚¤ì›Œë“œ: {kws}\n"
        result_report += "-" * 65 + "\n"
        
        stores_data.append({"name": p['name'], "lat": p['lat'], "lng": p['lng'], "rating": p['rating'], "address": p['address']})

    return {"result": result_report, "stores": stores_data, "scanned_count": scanned_count, "analyzed_count": analyzed_count}