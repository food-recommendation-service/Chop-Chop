import requests
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import re
import time
import os
import math
from dotenv import load_dotenv

# [ë³´ì•ˆ ë° ê²½ë¡œ ê°•í™”] í˜„ì¬ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ .env ë¡œë“œ
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

# ==================================================================================
# [1] ì„¤ì • ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (BOM ì œê±° ê¸°ëŠ¥ ì¶”ê°€)
# ==================================================================================
def clean_api_key(key):
    if not key: return ""
    # 1. ê³µë°± ì œê±°
    key = key.strip()
    # 2. ë³´ì´ì§€ ì•ŠëŠ” íŠ¹ìˆ˜ë¬¸ì(BOM ë“±) ë° í•œê¸€ ê°•ì œ ì œê±° (ìˆœìˆ˜ ì•„ìŠ¤í‚¤ë§Œ ë‚¨ê¹€)
    try:
        return key.encode('ascii', 'ignore').decode('ascii')
    except:
        return ""

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ í›„ í´ë¦¬ë‹ í•¨ìˆ˜ í†µê³¼
raw_google_key = os.getenv("GOOGLE_MAPS_API_KEY") or os.getenv("GOOGLE_API_KEY")
raw_gemini_key = os.getenv("GEMINI_API_KEY")

GOOGLE_API_KEY = clean_api_key(raw_google_key)
GEMINI_API_KEY = clean_api_key(raw_gemini_key)

# ë””ë²„ê¹…: í‚¤ê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì• 5ìë¦¬ë§Œ ì¶œë ¥)
print(f"ğŸ”‘ êµ¬ê¸€ í‚¤ ë¡œë“œ í™•ì¸: {GOOGLE_API_KEY[:5]}..." if GOOGLE_API_KEY else "ğŸš¨ êµ¬ê¸€ í‚¤ ì—†ìŒ")
print(f"ğŸ”‘ ì œë¯¸ë‚˜ì´ í‚¤ ë¡œë“œ í™•ì¸: {GEMINI_API_KEY[:5]}..." if GEMINI_API_KEY else "ğŸš¨ ì œë¯¸ë‚˜ì´ í‚¤ ì—†ìŒ")

if not GOOGLE_API_KEY:
    print("ğŸš¨ [ERROR] GOOGLE_API_KEYê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    llm_model = genai.GenerativeModel('models/gemini-2.0-flash')
else:
    print("ğŸš¨ [WARNING] GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. LLM ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ (ko-sroberta)...")
embed_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")

# ==================================================================================
# [2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==================================================================================

def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371  # ì§€êµ¬ ë°˜ì§€ë¦„ (km)
    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)
    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)
    dlon, dlat = lon2_rad - lon1_rad, lat2_rad - lat1_rad
    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

def get_naver_style_features(place_name, reviews):
    if not reviews or not GEMINI_API_KEY: return {}
    
    combined_review = " ".join([r.get('text', {}).get('text', '') for r in reviews[:5]])
    prompt = f"""
    ì‹ë‹¹ëª…: {place_name}
    ë¦¬ë·°: {combined_review[:800]}
    ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”: {{"atmosphere": "...", "companion": "...", "purpose": "...", "keywords": [...]}}
    """
    try:
        response = llm_model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except:
        return {}

# ==================================================================================
# [3] í•µì‹¬ ë¡œì§ (ìˆ˜ì§‘ -> í•„í„°ë§ -> ìŠ¤ì½”ì–´ë§)
# ==================================================================================

def get_bulk_places(search_query, center_lat, center_lng, radius_km):
    if not GOOGLE_API_KEY:
        return []

    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_API_KEY,  # .strip() ì²˜ë¦¬ëœ ê¹¨ë—í•œ í‚¤ ì‚¬ìš©
        'X-Goog-FieldMask': 'places.id,places.displayName,places.rating,places.userRatingCount,places.reviews,places.location,places.formattedAddress,nextPageToken'
    }
    
    places_list = []
    next_token = None
    
    # ë””ë²„ê¹…: ìš”ì²­ ì •ë³´ ì¶œë ¥
    print(f"ğŸ“¡ API ìš”ì²­: [{search_query}] | ìœ„ì¹˜: ({center_lat}, {center_lng}) | ë°˜ê²½: {radius_km}km")

    for i in range(3):
        payload = {
            "textQuery": search_query,
            "locationBias": {
                "circle": {
                    "center": {"latitude": float(center_lat), "longitude": float(center_lng)},
                    "radius": float(radius_km) * 1000  # ë¯¸í„° ë‹¨ìœ„ ë³€í™˜
                }
            },
            "languageCode": "ko", 
            "maxResultCount": 20, 
            "pageToken": next_token
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            
            if response.status_code != 200:
                print(f"âŒ API ì—ëŸ¬ ({response.status_code}): {response.text}")
                break
                
            resp_data = response.json()
            batch = resp_data.get('places', [])
            places_list.extend(batch)
            
            print(f"   â”” í˜ì´ì§€ {i+1}: {len(batch)}ê°œ í™•ë³´")
            
            next_token = resp_data.get('nextPageToken')
            if not next_token: break
            time.sleep(1.0) # API í• ë‹¹ëŸ‰ ì¤€ìˆ˜ ë° ê³¼ì—´ ë°©ì§€
        except Exception as e:
            print(f"ğŸš¨ ì‹œìŠ¤í…œ ì—ëŸ¬ ë°œìƒ: {e}")
            break
            
    return places_list

def hybrid_filter_similarity(place_docs, user_query, threshold=0.15):
    if not place_docs: return []
    
    doc_texts = [f"{p['name']} {p['text']}" for p in place_docs]
    embeddings = embed_model.encode(doc_texts)
    query_emb = embed_model.encode([user_query])
    sim_scores = cosine_similarity(query_emb, embeddings)[0]
    
    passed = []
    clean_query = user_query.replace(" ", "")
    
    for i, score in enumerate(sim_scores):
        p = place_docs[i]
        # Rule 1: í‚¤ì›Œë“œ ì§ì ‘ í¬í•¨ ì‹œ ê°€ì 
        if clean_query in p['name'].replace(" ","") or clean_query in p['text'].replace(" ",""):
            p['sim_score'] = max(score, 0.6)
            p['filter_reason'] = "Keyword Match"
            passed.append(p)
            continue
        # Rule 2: ë²¡í„° ìœ ì‚¬ë„ ê¸°ì¤€
        if score >= threshold:
            p['sim_score'] = score
            p['filter_reason'] = "Vector Similarity"
            passed.append(p)
            
    print(f"âœ‚ï¸ í•„í„°ë§ ê²°ê³¼: {len(place_docs)}ê°œ ì¤‘ {len(passed)}ê°œ í†µê³¼")
    return passed

# ==================================================================================
# [4] ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==================================================================================

def search_and_analyze(categories, user_detail, lat, lng, radius_km):
    search_keywords = [f"{cat} ë§›ì§‘" for cat in categories]
    if user_detail: search_keywords.append(f"{user_detail} ë§›ì§‘")
    if not search_keywords: search_keywords = ["ë§›ì§‘"]
    
    search_keywords = list(set(search_keywords))
    print(f"ğŸ•µï¸ ìˆ˜ì§‘ ì‹œì‘: {search_keywords}")

    all_raw_places = {}
    for kw in search_keywords:
        batch = get_bulk_places(kw, lat, lng, radius_km)
        for p in batch:
            if p.get('id') not in all_raw_places:
                all_raw_places[p['id']] = p
        if len(all_raw_places) >= 150: break # ìµœëŒ€ ìˆ˜ì§‘ëŸ‰ ì¡°ì ˆ
    
    print(f"âœ… ì´ {len(all_raw_places)}ê°œ ìœ ë‹ˆí¬ ì‹ë‹¹ í™•ë³´")

    # ê±°ë¦¬ í•„í„°ë§ ë° ë°ì´í„° ì •ê·œí™”
    filtered_places = []
    for p in all_raw_places.values():
        loc = p.get('location', {})
        dist = haversine_distance(lat, lng, loc.get('latitude', 0), loc.get('longitude', 0))
        
        if dist <= radius_km:
            reviews = p.get('reviews', [])
            review_text = " ".join([r.get('text', {}).get('text', '') for r in reviews])
            filtered_places.append({
                "name": p.get('displayName', {}).get('text', 'ì´ë¦„ì—†ìŒ'),
                "rating": p.get('rating', 0), 
                "count": p.get('userRatingCount', 0),
                "reviews": reviews, 
                "text": review_text,
                "lat": loc.get('latitude'), 
                "lng": loc.get('longitude'), 
                "address": p.get('formattedAddress', '')
            })

    # í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ í•„í„°ë§ ì ìš©
    candidates = hybrid_filter_similarity(filtered_places, user_detail, threshold=0.15)
    
    if not candidates: 
        return {"result": "âŒ ì£¼ë³€ì—ì„œ ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "stores": []}

    # ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚° (ìœ ì‚¬ë„ + ë³„ì  + ë¦¬ë·°ìˆ˜ + ì¸ê¸° ì ìˆ˜)
    for p in candidates:
        pop_score = min(np.log10(p['count'] + 1) / 4.0, 1.0) if p['count'] else 0
        rec_score = min(len(p['reviews']) / 5.0, 1.0) if p['reviews'] else 0
        p['total_score'] = (p['sim_score'] * 0.3) + (p['rating']/5 * 0.35) + (pop_score * 0.25) + (rec_score * 0.1)
        p['match_rate'] = int(p['total_score'] * 100)

    top_3 = sorted(candidates, key=lambda x: x['total_score'], reverse=True)[:3]
    
    # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    report = f"\nğŸ† ì¶”ì²œ ë¦¬í¬íŠ¸ (í•„í„°ë§ í†µê³¼ {len(candidates)}ê°œ ì¤‘ Top 3)\n"
    stores_data = []
    
    for rank, p in enumerate(top_3, 1):
        feats = get_naver_style_features(p['name'], p['reviews'])
        report += f"ğŸ… {rank}ìœ„: {p['name']} (ë§¤ì¹­ {p['match_rate']}%)\n"
        report += f"   âœ¨ {feats.get('purpose', 'ë§›ì§‘')} | {feats.get('atmosphere', 'ë¶„ìœ„ê¸° ì¢‹ìŒ')}\n"
        report += f"   ğŸ”‘ {', '.join(feats.get('keywords', []))}\n"
        stores_data.append({
            "name": p['name'], "lat": p['lat'], "lng": p['lng'], 
            "rating": p['rating'], "address": p['address'],
            "match_rate": p['match_rate']
        })

    return {
        "result": report, 
        "stores": stores_data, 
        "scanned_count": len(filtered_places), 
        "analyzed_count": len(candidates)
    }