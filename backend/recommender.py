import requests
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import re
import time
import os
import math
from dotenv import load_dotenv

load_dotenv()

# ==================================================================================
# [1] ì„¤ì •
# ==================================================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY", "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s")

genai.configure(api_key=GEMINI_API_KEY)
llm_model = genai.GenerativeModel('models/gemini-2.0-flash')

print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
embed_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")

# ==================================================================================
# [2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==================================================================================

def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371
    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)
    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)
    dlon, dlat = lon2_rad - lon1_rad, lat2_rad - lat1_rad
    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

def get_naver_style_features(place_name, reviews):
    if not reviews: return {}
    combined_review = " ".join([r.get('text', {}).get('text', '') for r in reviews[:5]])
    prompt = f"""
    ì‹ë‹¹ëª…: {place_name}
    ë¦¬ë·°: {combined_review[:800]}
    ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”: {{"atmosphere": "...", "companion": "...", "purpose": "...", "keywords": [...]}}
    """
    try:
        response = llm_model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except: return {}

# ==================================================================================
# [3] í•µì‹¬ ë¡œì§ (ìˆ˜ì§‘ -> í•„í„°ë§ -> ìŠ¤ì½”ì–´ë§)
# ==================================================================================

def get_bulk_places(search_query, center_lat, center_lng, radius_km):
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.rating,places.userRatingCount,places.reviews,places.location,places.formattedAddress,nextPageToken'
    }
    places_list = []
    next_token = None
    
    for _ in range(3):
        payload = {
            "textQuery": search_query,
            "locationBias": {"circle": {"center": {"latitude": center_lat, "longitude": center_lng}, "radius": radius_km * 1000}},
            "languageCode": "ko", "maxResultCount": 20, "pageToken": next_token
        }
        try:
            resp = requests.post(url, json=payload, headers=headers).json()
            batch = resp.get('places', [])
            places_list.extend(batch)
            next_token = resp.get('nextPageToken')
            if not next_token: break
            time.sleep(1.0)
        except: break
    return places_list

def hybrid_filter_similarity(place_docs, user_query, threshold=0.15):
    """
    [í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ ì—…ë°ì´íŠ¸]
    1. Rule-based: ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ì´ë¦„ì´ë‚˜ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ë©´ ë¬´ì¡°ê±´ í•©ê²© (ìµœì†Œ ì ìˆ˜ ë³´ì •)
    2. Vector-based: ì„ë² ë”© ìœ ì‚¬ë„ê°€ threshold ì´ìƒì´ë©´ í•©ê²©
    """
    if not place_docs: return []
    
    # ê°€ê²Œ ì´ë¦„ + ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•©
    doc_texts = [f"{p['name']} {p['text']}" for p in place_docs]
    
    embeddings = embed_model.encode(doc_texts)
    query_emb = embed_model.encode([user_query])
    sim_scores = cosine_similarity(query_emb, embeddings)[0]
    
    passed = []
    clean_query = user_query.replace(" ", "") # ê³µë°± ì œê±° ë¹„êµìš©
    
    for i, score in enumerate(sim_scores):
        p = place_docs[i]
        
        # [Rule 1] ì§ì ‘ì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­ (ì´ë¦„ì´ë‚˜ ë¦¬ë·°ì— ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°)
        if clean_query in p['name'].replace(" ","") or clean_query in p['text'].replace(" ",""):
            p['sim_score'] = max(score, 0.6) # ê²€ìƒ‰ì–´ í¬í•¨ ì‹œ ì ìˆ˜ ë³´ì • (0.6 ë¯¸ë§Œì´ì–´ë„ í•©ê²©)
            p['filter_reason'] = "Keyword Match"
            passed.append(p)
            continue
            
        # [Rule 2] ë²¡í„° ìœ ì‚¬ë„ ë§¤ì¹­ (ê´€ëŒ€í•œ ê¸°ì¤€ 0.15)
        if score >= threshold:
            p['sim_score'] = score
            p['filter_reason'] = "Vector Similarity"
            passed.append(p)
            
    print(f"âœ‚ï¸ í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§: {len(place_docs)}ê°œ ì¤‘ {len(passed)}ê°œ ìƒì¡´")
    return passed

# ==================================================================================
# [4] ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==================================================================================

def search_and_analyze(categories, user_detail, lat, lng, radius_km):
    search_keywords = [f"{cat} ë§›ì§‘" for cat in categories]
    if user_detail: search_keywords.append(f"{user_detail} ë§›ì§‘")
    if not search_keywords: search_keywords = ["ë§›ì§‘"]
    
    search_keywords = list(set(search_keywords))
    
    print(f"ğŸ•µï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords} ìˆ˜ì§‘ ì‹œì‘...")

    all_raw_places = {}
    for kw in search_keywords:
        batch = get_bulk_places(kw, lat, lng, radius_km)
        for p in batch:
            if p.get('id') not in all_raw_places:
                all_raw_places[p['id']] = p
        if len(all_raw_places) >= 200: break
    
    print(f"âœ… ì´ {len(all_raw_places)}ê°œ ì‹ë‹¹ í™•ë³´")

    filtered_places = []
    for p in all_raw_places.values():
        loc = p.get('location', {})
        dist = haversine_distance(lat, lng, loc.get('latitude', 0), loc.get('longitude', 0))
        
        if dist <= radius_km:
            reviews = p.get('reviews', [])
            review_text = " ".join([r.get('text', {}).get('text', '') for r in reviews])
            filtered_places.append({
                "name": p.get('displayName', {}).get('text', 'ì´ë¦„ì—†ìŒ'),
                "rating": p.get('rating', 0), "count": p.get('userRatingCount', 0),
                "reviews": reviews, "text": review_text,
                "lat": loc.get('latitude'), "lng": loc.get('longitude'), "address": p.get('formattedAddress', '')
            })

    # [í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ìœ¼ë¡œ êµì²´]
    candidates = hybrid_filter_similarity(filtered_places, user_detail, threshold=0.15)
    
    if not candidates: return {"result": "âŒ ê´€ë ¨ ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "stores": []}

    for p in candidates:
        pop_score = min(np.log10(p['count'] + 1) / 4.0, 1.0) if p['count'] else 0
        rec_score = min(len(p['reviews']) / 5.0, 1.0) if p['reviews'] else 0
        p['total_score'] = (p['sim_score'] * 0.3) + (p['rating']/5 * 0.35) + (pop_score * 0.25) + (rec_score * 0.1)
        p['match_rate'] = int(p['total_score'] * 100)

    top_3 = sorted(candidates, key=lambda x: x['total_score'], reverse=True)[:3]
    
    report = f"\n{'='*60}\nğŸ† ì¶”ì²œ ë¦¬í¬íŠ¸ (í•„í„°ë§ í†µê³¼ {len(candidates)}ê°œ ì¤‘ Top 3)\n{'='*60}\n"
    stores_data = []
    
    for rank, p in enumerate(top_3, 1):
        feats = get_naver_style_features(p['name'], p['reviews'])
        report += f"ğŸ… {rank}ìœ„: {p['name']} (ë§¤ì¹­ {p['match_rate']}%)\n"
        report += f"   âœ¨ {feats.get('purpose', 'ë§›ì§‘')} | {feats.get('atmosphere', 'ë¶„ìœ„ê¸° ì¢‹ìŒ')}\n"
        report += f"   ğŸ”‘ {', '.join(feats.get('keywords', []))}\n"
        report += "-"*60 + "\n"
        stores_data.append({"name": p['name'], "lat": p['lat'], "lng": p['lng'], "rating": p['rating'], "address": p['address']})

    return {"result": report, "stores": stores_data, "scanned_count": len(filtered_places), "analyzed_count": len(candidates)}