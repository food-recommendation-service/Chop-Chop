import requests
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import re
import time
import os
import math
from dotenv import load_dotenv
from mapping_utils import map_google_to_yelp_style

# 1. í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì •
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

def clean_api_key(key):
    if not key: return ""
    try: return key.strip().encode('ascii', 'ignore').decode('ascii')
    except: return ""

GOOGLE_API_KEY = clean_api_key(os.getenv("GOOGLE_MAPS_API_KEY") or os.getenv("GOOGLE_API_KEY"))
GEMINI_API_KEY = clean_api_key(os.getenv("GEMINI_API_KEY"))

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    llm_model = genai.GenerativeModel('models/gemini-2.0-flash')

embed_model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))

def get_naver_style_features(place_name, reviews):
    if not reviews or not GEMINI_API_KEY: return {}
    combined_review = " ".join([r.get('text', {}).get('text', '') for r in reviews[:5]])
    prompt = f"ì‹ë‹¹ëª…: {place_name}\në¦¬ë·°: {combined_review[:800]}\nì •ë³´ JSON ì¶”ì¶œ: {{'atmosphere': '...', 'purpose': '...', 'keywords': [...]}}"
    try:
        response = llm_model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except: return {}

def get_bulk_places(search_query, center_lat, center_lng, radius_km):
    if not GOOGLE_API_KEY: return []
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.rating,places.userRatingCount,places.reviews,places.location,places.formattedAddress,places.editorialSummary,places.priceLevel,places.servesBeer,places.servesWine,places.parkingOptions,places.goodForGroups,places.menuForChildren,places.accessibilityOptions,places.outdoorSeating,places.dineIn,places.servesCocktails,places.servesVegetarianFood,nextPageToken'
    }
    places_list = []
    next_token = None
    for i in range(3):
        payload = {
            "textQuery": search_query,
            "locationBias": {"circle": {"center": {"latitude": center_lat, "longitude": center_lng}, "radius": radius_km*1000}},
            "languageCode": "ko", "maxResultCount": 20, "pageToken": next_token
        }
        try:
            resp = requests.post(url, json=payload, headers=headers)
            data = resp.json()
            places_list.extend(data.get('places', []))
            next_token = data.get('nextPageToken')
            if not next_token: break
            time.sleep(1)
        except: break
    return places_list

def hybrid_filter_similarity(place_docs, user_query, threshold=0.15):
    if not place_docs: return []
    doc_texts = [f"{p['name']} {p['text']}" for p in place_docs]
    embeddings = embed_model.encode(doc_texts)
    query_emb = embed_model.encode([user_query])
    sim_scores = cosine_similarity(query_emb, embeddings)[0]
    passed = []
    clean_query = user_query.replace(" ", "")
    for i, score in enumerate(sim_scores):
        p = place_docs[i]
        if clean_query in p['name'].replace(" ","") or clean_query in p['text'].replace(" ",""):
            p['sim_score'] = max(score, 0.6)
            passed.append(p)
        elif score >= threshold:
            p['sim_score'] = score
            passed.append(p)
    return passed

# âœ… 3. ë©”ì¸ íŒŒì´í”„ë¼ì¸ (ë””ë²„ê·¸ ë¡œê·¸ + ì•ˆì „ì¥ì¹˜ ì¶”ê°€)
def search_and_analyze(categories, user_detail, lat, lng, radius_km, filters=None):
    # 1. ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
    search_keywords = list(set([f"{cat} ë§›ì§‘" for cat in categories] + ([f"{user_detail} ë§›ì§‘"] if user_detail else [])))
    
    # 2. ë°ì´í„° ìˆ˜ì§‘
    print(f"ğŸ•µï¸ ìˆ˜ì§‘ ì‹œì‘: {search_keywords}")
    all_raw_places = {}
    for kw in search_keywords:
        batch = get_bulk_places(kw, lat, lng, radius_km)
        for p in batch:
            if p.get('id') not in all_raw_places:
                all_raw_places[p['id']] = p
        if len(all_raw_places) >= 150: 
            break
    
    print(f"âœ… ì´ {len(all_raw_places)}ê°œ ìœ ë‹ˆí¬ ì‹ë‹¹ í™•ë³´")

    # 3. [Stage 1] ë°˜ê²½ ë° í•˜ë“œ í•„í„°ë§
    print("\nğŸ” [Stage 1] ë°˜ê²½ ë° í•˜ë“œ í•„í„°ë§ ì§„í–‰ ì¤‘...")
    filtered_places = []
    hard_dropped_count = 0
    radius_dropped_count = 0

    safe_filters = filters if isinstance(filters, dict) else (dict(filters) if filters else {})
    if safe_filters:
        print(f"ğŸ› [DEBUG] ì ìš©ë  í•„ìˆ˜ í•„í„°: {safe_filters}")

    for p in all_raw_places.values():
        loc = p.get('location', {})
        dist = haversine_distance(lat, lng, loc.get('latitude', 0), loc.get('longitude', 0))
        
        # ë°˜ê²½ ì²´í¬
        if dist > radius_km:
            radius_dropped_count += 1
            continue
        
        # í•˜ë“œ í•„í„° ì²´í¬
        yelp_style_attr = map_google_to_yelp_style(p)
        is_match = True
        drop_reason = ""
        
        if safe_filters:
            for key, required in safe_filters.items():
                if required == 1 and yelp_style_attr.get(key) == 0:
                    is_match = False
                    drop_reason = key
                    break
        
        if not is_match: 
            print(f"   ğŸš« [ì œì™¸ë¨] {p.get('displayName', {}).get('text')} (ì´ìœ : {drop_reason} ì—†ìŒ)")
            hard_dropped_count += 1
            continue 

        reviews = p.get('reviews', [])
        review_text = " ".join([r.get('text', {}).get('text', '') for r in reviews])
        
        filtered_places.append({
            "name": p.get('displayName', {}).get('text', 'ì´ë¦„ì—†ìŒ'),
            "rating": p.get('rating', 0), 
            "count": p.get('userRatingCount', 0),
            "reviews": reviews, 
            "text": review_text,
            "lat": loc.get('latitude'), 
            "lng": loc.get('longitude'), 
            "address": p.get('formattedAddress', ''),
            "summary": p.get('editorialSummary', {}).get('text', ''),
            "yelp_attrs": yelp_style_attr
        })

    print(f"âœ‚ï¸ ë°˜ê²½ í•„í„°ë§ ê²°ê³¼: ì´ {radius_dropped_count}ê°œ ì‹ë‹¹ ë°˜ê²½ ì´ˆê³¼ë¡œ ì œì™¸ë¨.")
    print(f"âœ‚ï¸ í•˜ë“œ í•„í„°ë§ ê²°ê³¼: ì´ {hard_dropped_count}ê°œ ì‹ë‹¹ í•„ìˆ˜ ì˜µì…˜ ë¯¸ë‹¬ë¡œ ì œì™¸ë¨.")
    print(f"âœ… Stage 1 í†µê³¼ ì‹ë‹¹: {len(filtered_places)}ê°œ")

    # 4. ğŸ”¥ [Stage 2] ì†Œí”„íŠ¸ í•„í„°ë§ (ì„ë² ë”© ìœ ì‚¬ë„ ë¶„ì„) - ì—¬ê¸°ê°€ ë¹ ì ¸ì„œ ì—ëŸ¬ ë‚¬ë˜ ê²ƒ!
    print("\nğŸ§  [Stage 2] ì†Œí”„íŠ¸ í•„í„°ë§(ìœ ì‚¬ë„ ë¶„ì„) ì§„í–‰ ì¤‘...")
    threshold = 0.01 
    candidates = []
    soft_dropped_count = 0

    if filtered_places:
        doc_texts = [f"{p['name']} {p['text']}" for p in filtered_places]
        embeddings = embed_model.encode(doc_texts)
        query_text = user_detail if user_detail else " ".join(categories)
        query_emb = embed_model.encode([query_text])
        sim_scores = cosine_similarity(query_emb, embeddings)[0]

        for i, score in enumerate(sim_scores):
            p = filtered_places[i]
            p['sim_score'] = float(score)
            
            if score >= threshold:
                candidates.append(p)
            else:
                print(f"   ğŸ“‰ [ìœ ì‚¬ë„ íƒˆë½] {p['name']} (ì ìˆ˜: {score:.4f} < ê¸°ì¤€: {threshold})")
                soft_dropped_count += 1
    
    print(f"âœ‚ï¸ ì†Œí”„íŠ¸ í•„í„°ë§ ê²°ê³¼: ì´ {soft_dropped_count}ê°œ ì‹ë‹¹ ì œì™¸ë¨.")
    print(f"âœ… ìµœì¢… í›„ë³´êµ°: {len(candidates)}ê°œ ì‹ë‹¹ (ìŠ¤ì½”ì–´ë§ ëŒ€ìƒ)")

    if not candidates: 
        return {"result": "âŒ ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.", "stores": []}

    # 5. ìŠ¤ì½”ì–´ë§ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°
    print("\nğŸ“Š [Scoring] ê° í›„ë³´ ì ìˆ˜ ë¶„í•´:")
    for p in candidates:
        pop_score = min(np.log10(p['count'] + 1) / 4.0, 1.0) if p['count'] else 0
        rec_score = min(len(p['reviews']) / 5.0, 1.0) if p['reviews'] else 0

        s_sim    = p['sim_score'] * 0.6
        s_rating = (p['rating'] / 5) * 0.2
        s_pop    = pop_score * 0.15
        s_rec    = rec_score * 0.05

        p['total_score'] = s_sim + s_rating + s_pop + s_rec
        p['match_rate'] = int(p['total_score'] * 100)

        # ì–´ë–¤ ìš”ì†Œê°€ ì ìˆ˜ë¥¼ ê°€ì¥ ë§ì´ ì˜¬ë ¸ëŠ”ì§€ dominant factor ê³„ì‚°
        factor_map = {
            "ìœ ì‚¬ë„": s_sim,
            "ë³„ì ": s_rating,
            "ë¦¬ë·°ìˆ˜(ì¸ê¸°)": s_pop,
            "ë¦¬ë·°ì‹ ë¢°ë„": s_rec
        }
        dominant = max(factor_map, key=factor_map.get)

        print(
            f"   ğŸ“Œ {p['name']:<20} | "
            f"ìœ ì‚¬ë„: {p['sim_score']:.3f}â†’{s_sim:.3f}  "
            f"ë³„ì : {p['rating']:.1f}â†’{s_rating:.3f}  "
            f"ì¸ê¸°: {p['count']}ê±´â†’{s_pop:.3f}  "
            f"ì‹ ë¢°ë„: {s_rec:.3f}  "
            f"í•©ê³„: {p['total_score']:.3f}  "
            f"ğŸ†dominant: {dominant}"
        )

    # 6. ìƒìœ„ 15ê°œ ì¶”ì¶œ
    top_candidates = sorted(candidates, key=lambda x: x['total_score'], reverse=True)[:15]

    print(f"\nğŸ¥‡ [Top {len(top_candidates)} í™•ì •]")
    for rank, p in enumerate(top_candidates, 1):
        print(f"   {rank}ìœ„: {p['name']} ({p['match_rate']}%)")

    # 7. ë¦¬í¬íŠ¸ ìƒì„±
    report = f"\nğŸ† ì¶”ì²œ ë¦¬í¬íŠ¸ (í†µê³¼ {len(candidates)}ê°œ ì¤‘ ìƒìœ„ {len(top_candidates)}ê°œ)\n"
    stores_data = []
    for rank, p in enumerate(top_candidates, 1):
        feats = get_naver_style_features(p['name'], p['reviews']) if rank <= 5 else {}
        report += f"ğŸ… {rank}ìœ„: {p['name']} (ë§¤ì¹­ {p['match_rate']}%)\n"
        if feats:
            report += f"   âœ¨ {feats.get('purpose', 'ë§›ì§‘')} | {feats.get('atmosphere', 'ë¶„ìœ„ê¸° ì¢‹ìŒ')}\n"

        stores_data.append({
            "name": p['name'],
            "lat": p['lat'],
            "lng": p['lng']
        })
    
    return {
        "result": report,
        "stores": stores_data,
        "scanned_count": len(all_raw_places),
        "analyzed_count": len(candidates)
    }