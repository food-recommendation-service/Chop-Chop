import requests
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json
import re
import time
import os
from dotenv import load_dotenv

load_dotenv()

# ==================================================================================
# [1] ì„¤ì •
# ==================================================================================
GOOGLE_API_KEY = os.getenv("GOOGLE_MAPS_API_KEY", "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyC-gSjkrWo8mjx8N_NR4h6a6Bk7taseW7s")

genai.configure(api_key=GEMINI_API_KEY)
llm_model = genai.GenerativeModel('models/gemini-2.0-flash')

print("â³ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
embed_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
print("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n")

# ==================================================================================
# [2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==================================================================================

def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371
    lat1_rad, lon1_rad = np.radians(lat1), np.radians(lon1)
    lat2_rad, lon2_rad = np.radians(lat2), np.radians(lon2)
    dlon, dlat = lon2_rad - lon1_rad, lat2_rad - lat1_rad
    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

def get_naver_style_features(place_name, reviews):
    if not reviews: return {}
    combined_review = " ".join([r.get('text', {}).get('text', '') for r in reviews[:5]])
    prompt = f"""
    ì‹ë‹¹ëª…: {place_name}
    ë¦¬ë·°: {combined_review[:800]}
    ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”: {{"atmosphere": "...", "companion": "...", "purpose": "...", "keywords": [...]}}
    """
    try:
        response = llm_model.generate_content(prompt)
        match = re.search(r'\{.*\}', response.text, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except: return {}

# ==================================================================================
# [3] í•µì‹¬ ë¡œì§ (ìˆ˜ì§‘ -> í•„í„°ë§ -> ìŠ¤ì½”ì–´ë§)
# ==================================================================================

def get_bulk_places(search_query, center_lat, center_lng, radius_km):
    """
    êµ¬ê¸€ API í˜¸ì¶œ í•¨ìˆ˜ (ë‹¨ì¼ í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 60ê°œ)
    ê¹”ë”í•˜ê²Œ API í˜¸ì¶œ ì—­í• ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        'Content-Type': 'application/json',
        'X-Goog-Api-Key': GOOGLE_API_KEY,
        'X-Goog-FieldMask': 'places.id,places.displayName,places.rating,places.userRatingCount,places.reviews,places.location,places.formattedAddress,nextPageToken'
    }
    places_list = []
    next_token = None
    
    # 3í˜ì´ì§€(60ê°œ)ê¹Œì§€ë§Œ ìˆ˜ì§‘ (êµ¬ê¸€ ì œí•œ)
    for _ in range(3):
        payload = {
            "textQuery": search_query,
            "locationBias": {"circle": {"center": {"latitude": center_lat, "longitude": center_lng}, "radius": radius_km * 1000}},
            "languageCode": "ko", "maxResultCount": 20, "pageToken": next_token
        }
        try:
            resp = requests.post(url, json=payload, headers=headers).json()
            batch = resp.get('places', [])
            places_list.extend(batch)
            next_token = resp.get('nextPageToken')
            if not next_token: break
            time.sleep(1.0)
        except: break
    return places_list

def hard_filter_by_similarity(place_docs, user_query, threshold=0.3):
    """
    í•˜ë“œ í•„í„°ë§: ê°€ê²Œ ì´ë¦„ + ë¦¬ë·° í…ìŠ¤íŠ¸ë¡œ ìœ ì‚¬ë„ ê²€ì‚¬
    """
    if not place_docs: return []
    
    # [ì¤‘ìš” ë³´ì™„] ë¦¬ë·°ë¿ë§Œ ì•„ë‹ˆë¼ 'ê°€ê²Œ ì´ë¦„'ë„ ê°™ì´ ë´…ë‹ˆë‹¤. (í—ˆë‹ˆì½¤ë³´/ë°©ì–´ ì°¾ê¸° í•„ìˆ˜)
    doc_texts = [f"{p['name']} {p['text']}" for p in place_docs]
    
    embeddings = embed_model.encode(doc_texts)
    query_emb = embed_model.encode([user_query])
    sim_scores = cosine_similarity(query_emb, embeddings)[0]
    
    passed = []
    for i, score in enumerate(sim_scores):
        if score >= threshold:
            place_docs[i]['sim_score'] = score
            passed.append(place_docs[i])
            
    print(f"âœ‚ï¸ í•˜ë“œ í•„í„°ë§: {len(place_docs)}ê°œ ì¤‘ {len(passed)}ê°œ ìƒì¡´ (ê¸°ì¤€: {threshold})")
    return passed

# ==================================================================================
# [4] ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==================================================================================

def search_and_analyze(categories, user_detail, lat, lng, radius_km):
    # 1. ê²€ìƒ‰ì–´ ì¤€ë¹„ (200ê°œ ìˆ˜ì§‘ì„ ìœ„í•œ í‚¤ì›Œë“œ ë¶„í• )
    search_keywords = [f"{cat} ë§›ì§‘" for cat in categories]
    if user_detail: search_keywords.append(f"{user_detail} ë§›ì§‘")
    if not search_keywords: search_keywords = ["ë§›ì§‘"]
    
    search_keywords = list(set(search_keywords)) # ì¤‘ë³µ ì œê±°
    
    print(f"ğŸ•µï¸ ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords} ë¡œ ìˆ˜ì§‘ ì‹œì‘...")

    # 2. 200ê°œ ë°ì´í„° ìˆ˜ì§‘ (í‚¤ì›Œë“œë³„ë¡œ ëŒë©´ì„œ ëª¨ìœ¼ê¸°)
    all_raw_places = {}
    for kw in search_keywords:
        batch = get_bulk_places(kw, lat, lng, radius_km)
        for p in batch:
            if p.get('id') not in all_raw_places:
                all_raw_places[p['id']] = p
        if len(all_raw_places) >= 200: break # 200ê°œ ì°¨ë©´ ì¤‘ë‹¨
    
    print(f"âœ… ì´ {len(all_raw_places)}ê°œ ì‹ë‹¹ í™•ë³´ ì™„ë£Œ")

    # 3. ì „ì²˜ë¦¬ (ê±°ë¦¬ ê³„ì‚° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ)
    filtered_places = []
    for p in all_raw_places.values():
        loc = p.get('location', {})
        dist = haversine_distance(lat, lng, loc.get('latitude', 0), loc.get('longitude', 0))
        
        if dist <= radius_km:
            reviews = p.get('reviews', [])
            # ë¹ˆ í…ìŠ¤íŠ¸ë¼ë„ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰ë˜ê²Œ ì²˜ë¦¬
            review_text = " ".join([r.get('text', {}).get('text', '') for r in reviews])
            filtered_places.append({
                "name": p.get('displayName', {}).get('text', 'ì´ë¦„ì—†ìŒ'),
                "rating": p.get('rating', 0), "count": p.get('userRatingCount', 0),
                "reviews": reviews, "text": review_text,
                "lat": loc.get('latitude'), "lng": loc.get('longitude'), "address": p.get('formattedAddress', '')
            })

    # 4. í•˜ë“œ í•„í„°ë§ (ì‚¬ìš©ì ìƒì„¸ ì…ë ¥ ê¸°ì¤€)
    # ë¦¬ë·°ê°€ ì—†ì–´ë„ ê°€ê²Œ ì´ë¦„ìœ¼ë¡œ ì‚´ë¦¬ê¸° ìœ„í•´ ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬ ì œê±°
    candidates = hard_filter_by_similarity(filtered_places, user_detail, threshold=0.3)
    
    if not candidates: return {"result": "âŒ ê´€ë ¨ ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "stores": []}

    # 5. ìŠ¤ì½”ì–´ë§ (í‰ì , ë¦¬ë·°ìˆ˜, ìœ ì‚¬ë„ ë°˜ì˜)
    for p in candidates:
        # ì¸ê¸°ë„ ì ìˆ˜ ê³„ì‚° (ë¡œê·¸ ìŠ¤ì¼€ì¼)
        pop_score = min(np.log10(p['count'] + 1) / 4.0, 1.0) if p['count'] else 0
        # ìµœì‹ ì„± ì ìˆ˜
        rec_score = min(len(p['reviews']) / 5.0, 1.0) if p['reviews'] else 0
        
        p['total_score'] = (p['sim_score'] * 0.3) + (p['rating']/5 * 0.35) + (pop_score * 0.25) + (rec_score * 0.1)
        p['match_rate'] = int(p['total_score'] * 100)

    # 6. ìƒìœ„ 3ê°œ ì„ ì • ë° ë¦¬í¬íŠ¸
    top_3 = sorted(candidates, key=lambda x: x['total_score'], reverse=True)[:3]
    
    report = f"\n{'='*60}\nğŸ† ì¶”ì²œ ë¦¬í¬íŠ¸ ({len(candidates)}ê°œ í›„ë³´ ì¤‘ Top 3)\n{'='*60}\n"
    stores_data = []
    
    for rank, p in enumerate(top_3, 1):
        feats = get_naver_style_features(p['name'], p['reviews'])
        report += f"ğŸ… {rank}ìœ„: {p['name']} (ë§¤ì¹­ {p['match_rate']}%)\n"
        report += f"   âœ¨ {feats.get('purpose', 'ë§›ì§‘')} | {feats.get('atmosphere', 'ë¶„ìœ„ê¸° ì¢‹ìŒ')}\n"
        report += f"   ğŸ”‘ {', '.join(feats.get('keywords', []))}\n"
        report += "-"*60 + "\n"
        stores_data.append({"name": p['name'], "lat": p['lat'], "lng": p['lng'], "rating": p['rating'], "address": p['address']})

    return {"result": report, "stores": stores_data, "scanned_count": len(filtered_places), "analyzed_count": len(candidates)}